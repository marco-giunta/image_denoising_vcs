{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfw(Dataset):\n",
    "    def __init__(self, base_path: str = './data/', photo_length: int = 112, subtract_mean: bool = False, divide_std: bool = False, device: str = 'cpu'): #, divide_255: bool = True\n",
    "        self.cropped_file_path = base_path + 'cropped/'\n",
    "        self.pixelated_file_path = base_path + 'pixelated/'\n",
    "        self.blurred_file_path = base_path + 'blurred/'\n",
    "        # self.tensor_file_path = base_path + 'tensor/'\n",
    "\n",
    "        self.n_files = len(os.listdir(self.cropped_file_path))\n",
    "        self.photo_length = photo_length\n",
    "        \n",
    "        self.cropped = torch.empty((self.n_files, 3, self.photo_length, self.photo_length)) # torchvision mette i canali rgb al primo posto\n",
    "        self.pixelated = torch.empty((self.n_files, 3, self.photo_length, self.photo_length))\n",
    "        self.blurred = torch.empty((self.n_files, 3, self.photo_length, self.photo_length))\n",
    "\n",
    "        # if os.listdir(self.tensor_file_path) == []:\n",
    "        for i in range(self.n_files):\n",
    "            file_name = f'{i}.jpg'\n",
    "            self.cropped[i] = torchvision.io.read_image(self.cropped_file_path + file_name)\n",
    "            self.pixelated[i] = torchvision.io.read_image(self.pixelated_file_path + file_name)\n",
    "            self.blurred[i] = torchvision.io.read_image(self.blurred_file_path + file_name)\n",
    "\n",
    "            # torch.save(self.cropped, self.tensor_file_path + 'cropped.pt')\n",
    "            # torch.save(self.pixelated, self.tensor_file_path + 'pixelated.pt')\n",
    "            # torch.save(self.blurred, self.tensor_file_path + 'blurred.pt')\n",
    "\n",
    "        # else:\n",
    "        #     self.cropped = torch.load(self.tensor_file_path + 'cropped.pt')\n",
    "        #     self.pixelated = torch.load(self.tensor_file_path + 'pixelated.pt')\n",
    "        #     self.blurred = torch.load(self.tensor_file_path + 'blurred.pt')\n",
    "\n",
    "        if subtract_mean:\n",
    "            self.cropped -= self.cropped.mean()\n",
    "            self.pixelated -= self.pixelated.mean()\n",
    "            self.blurred -= self.blurred.mean()\n",
    "        \n",
    "        if divide_std:\n",
    "            self.cropped /= self.cropped.std()\n",
    "            self.pixelated /= self.pixelated.std()\n",
    "            self.blurred /= self.blurred.std()\n",
    "\n",
    "        self.cropped /= 255\n",
    "        self.pixelated /= 255\n",
    "        self.blurred /= 255\n",
    "\n",
    "        \n",
    "        if self.device == 'cuda':\n",
    "            self.cropped, self.pixelated, self.blurred = self.cropped.to('cuda'), self.pixelated.to('cuda'), self.blurred.to('cuda')\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.cropped[index], self.pixelated[index], self.blurred[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = lfw() # 2,5 - 3 minuti circa, 5-6 giga di ram occupati (senza cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d[6116][0].shape 3*112*112\n",
    "# per mettere il canale alla fine anziché all'inizio come richiesto ad es. da matplotlib si può usare np.transpose con l'argomento axes = (1, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/23943379/swapping-the-dimensions-of-a-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(d.cropped, './data/tensor/cropped.pt')\n",
    "# torch.save(d.pixelated, './data/tensor/pixelated.pt')\n",
    "# torch.save(d.blurred, './data/tensor/blurred.pt')\n",
    "# impiegano circa 20 secondi a testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2 = lfw() # 2 e mezzo senza tensori, poco meno di 2 minuti con. Tanto vale risparmiare 5,56 gb di memoria..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfw_b(Dataset):\n",
    "    def __init__(self, train: bool, training_frac = 0.8, base_path: str = './data/', photo_length: int = 112,\n",
    "                 subtract_mean: bool = False, divide_std: bool = False, device: str = 'cpu'): #, divide_255: bool = True\n",
    "        self.cropped_file_path = base_path + 'cropped/'\n",
    "        # self.pixelated_file_path = base_path + 'pixelated/'\n",
    "        self.blurred_file_path = base_path + 'blurred/'\n",
    "        # self.tensor_file_path = base_path + 'tensor/'\n",
    "\n",
    "        self.n_files = len(os.listdir(self.cropped_file_path)) # numero globale\n",
    "        self.photo_length = photo_length\n",
    "\n",
    "        train_index  = int(training_frac * self.n_files)\n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            self.n_files = train_index # solo train\n",
    "            faces_indices = range(train_index)\n",
    "        else:\n",
    "            faces_indices = range(train_index, self.n_files)\n",
    "            self.n_files = self.n_files - train_index # solo test ma dopo che ho usato quello globale la riga precedente\n",
    "        \n",
    "        self.cropped = torch.empty((self.n_files, 3, self.photo_length, self.photo_length)) # torchvision mette i canali rgb al primo posto\n",
    "        # self.pixelated = torch.empty((self.n_files, 3, self.photo_length, self.photo_length))\n",
    "        self.blurred = torch.empty((self.n_files, 3, self.photo_length, self.photo_length))\n",
    "\n",
    "        # faces_indices = range(train_index) if self.train else range(train_index, self.n_files)\n",
    "\n",
    "        for i, j in enumerate(faces_indices):\n",
    "            file_name = f'{j}.jpg'\n",
    "            self.cropped[i] = torchvision.io.read_image(self.cropped_file_path + file_name)\n",
    "            # self.pixelated[i] = torchvision.io.read_image(self.pixelated_file_path + file_name)\n",
    "            self.blurred[i] = torchvision.io.read_image(self.blurred_file_path + file_name)\n",
    "\n",
    "        if subtract_mean:\n",
    "            self.cropped -= self.cropped.mean()\n",
    "            # self.pixelated -= self.pixelated.mean()\n",
    "            self.blurred -= self.blurred.mean()\n",
    "        \n",
    "        if divide_std:\n",
    "            self.cropped /= self.cropped.std()\n",
    "            # self.pixelated /= self.pixelated.std()\n",
    "            self.blurred /= self.blurred.std()\n",
    "\n",
    "        self.cropped /= 255\n",
    "        # self.pixelated /= 255\n",
    "        self.blurred /= 255\n",
    "\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            # self.cropped, self.pixelated, self.blurred = self.cropped.to('cuda'), self.pixelated.to('cuda'), self.blurred.to('cuda')\n",
    "            self.cropped, self.blurred = self.cropped.to('cuda'), self.blurred.to('cuda')\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return self.cropped[index], self.pixelated[index], self.blurred[index]\n",
    "        return self.cropped[index], self.blurred[index]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur = lfw_b() # cuda + solo blurred: 2 minuti e qualcosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur.blurred.shape # 13233, 3, 112, 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = lfw_b(train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10586, 3, 112, 112])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8468"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(training_set.n_files*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = lfw_b(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfw_tensor(Dataset):\n",
    "    def __init__(self, cropped, pixelated, blurred):\n",
    "        self.cropped, self.pixelated, self.blurred = cropped, pixelated, blurred\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.cropped[index], self.pixelated[index], self.blurred[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cropped.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfw_tensor_b(Dataset):\n",
    "    def __init__(self, cropped, blurred):\n",
    "        self.cropped, self.blurred = cropped, blurred\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.cropped[index], self.blurred[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cropped.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create dataloaders from the above datasets in a grid-search friendly way, as batch_size is an argument\n",
    "def create_train_val_dataloaders(dataset: lfw, batch_size: int, training_frac = 0.8, num_workers: int = 0, seed: int = 1234):\n",
    "    np.random.seed(seed)\n",
    "    n_samples = len(dataset)\n",
    "    train_indices = np.random.choice(n_samples, size = int(training_frac * n_samples), replace = False)\n",
    "    val_indices   = np.setdiff1d(np.arange(n_samples), train_indices)\n",
    "\n",
    "    train_partial_dataset = lfw_tensor(dataset.cropped[train_indices], dataset.pixelated[train_indices], dataset.blurred[train_indices])\n",
    "    val_dataset = lfw_tensor(dataset.cropped[val_indices], dataset.pixelated[val_indices], dataset.blurred[val_indices])\n",
    "    \n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.random.manual_seed(seed)\n",
    "    train_dataloader = DataLoader(train_partial_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    val_dataloader   = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = num_workers)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_dataloaders_b(dataset: lfw, batch_size: int, training_frac = 0.8, num_workers: int = 0, seed: int = 1234):\n",
    "    np.random.seed(seed)\n",
    "    n_samples = len(dataset)\n",
    "    train_indices = np.random.choice(n_samples, size = int(training_frac * n_samples), replace = False)\n",
    "    val_indices   = np.setdiff1d(np.arange(n_samples), train_indices)\n",
    "\n",
    "    train_partial_dataset = lfw_tensor_b(dataset.cropped[train_indices], dataset.blurred[train_indices])\n",
    "    val_dataset = lfw_tensor_b(dataset.cropped[val_indices], dataset.blurred[val_indices])\n",
    "    \n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.random.manual_seed(seed)\n",
    "    train_dataloader = DataLoader(train_partial_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    val_dataloader   = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = num_workers)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividi train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarAutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_dim: int, activation_function: str, optimizer: str,\n",
    "                initial_lr: float, batch_size: int, dropout: float = None, l1_reg_strength: float = None, l2_reg_strength: float = None,\n",
    "                loss_fn = nn.MSELoss(reduction = 'sum'), device = 'cuda'):\n",
    "        super().__init__()\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.activation = getattr(nn, activation_function)()\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size = 4, stride = 3, padding = 1), #out = (8, 14, 14)\n",
    "            self.activation,\n",
    "            nn.Dropout2d(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Conv2d(8, 16, kernel_size = 3, stride = 2, padding = 1), #out = (16, 7, 7)\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Conv2d(16, 32, kernel_size = 3, stride = 2, padding = 0), #out = (32, 3, 3)\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Conv2d(32, 32, kernel_size = 3, stride = 2, padding = 0), #out = (32, 3, 3)\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Flatten(start_dim = 1)\n",
    "        )\n",
    "        # 288 = 3*3*32\n",
    "        self.decoder = nn.Sequential( \n",
    "            nn.Linear(self.latent_space_dim, 112),\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Linear(112, 512), # 288\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Unflatten(dim = 1, unflattened_size = (32, 4, 4)),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size = 3, stride = 2, output_padding = 0),\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size = 3, stride = 2, output_padding = 0),\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 2, padding = 1, output_padding = 0),\n",
    "            self.activation,\n",
    "            nn.Dropout2d(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.ConvTranspose2d(8, 3, kernel_size = 4, stride = 3, padding = 1, output_padding = 2),\n",
    "            nn.Sigmoid() #-> [0,1] output\n",
    "        )\n",
    "\n",
    "        self.avg = nn.Sequential( # Predicts the means of a MVN dist.\n",
    "            nn.Linear(512, 112), # 288\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Linear(112, self.latent_space_dim)\n",
    "        )\n",
    "\n",
    "        self.log_var = nn.Sequential( # Predicts the (log) variances of an uncorrelated MVN (i.e. log of cov. matrix diagonal)\n",
    "            nn.Linear(512, 112), # 288\n",
    "            self.activation,\n",
    "            nn.Dropout(self.dropout) if dropout is not None else nn.Identity(),\n",
    "            nn.Linear(112, self.latent_space_dim)\n",
    "        )\n",
    "\n",
    "        if optimizer == 'SGD': # if optimizer is SGD we add Nesterov momentum with a 0.9 fairly standard value\n",
    "            self.optimizer = torch.optim.SGD(params = self.parameters(), momentum = 0.9, nesterov = True, lr = initial_lr)\n",
    "        else:\n",
    "            self.optimizer = getattr(torch.optim, optimizer)(params = self.parameters(), lr = initial_lr)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.loss_fn = loss_fn\n",
    "        self.l1_reg_strength = l1_reg_strength\n",
    "        self.l2_reg_strength = l2_reg_strength\n",
    "\n",
    "        if l1_reg_strength is None and l2_reg_strength is None:\n",
    "            self.reg = lambda: torch.tensor([0], dtype = float, requires_grad = True).to(self.device)\n",
    "        if l1_reg_strength is not None:\n",
    "            self.reg = lambda: (sum(((w.abs()).sum() for w in self.parameters()))*self.l1_reg_strength).to(self.device)\n",
    "        if l2_reg_strength is not None:\n",
    "            self.reg = lambda: (sum(((w**2).sum() for w in self.parameters()))*self.l2_reg_strength).to(self.device)\n",
    "\n",
    "    def sample_in_latent_space(self, mu, log_var): # sampling from MVN in latent space with provided mean and variances\n",
    "        sigma = torch.exp(0.5 * log_var) #log_var is log(variance) = log(sigma**2) = 2 * log(sigma)\n",
    "        return mu + torch.randn_like(mu) * sigma\n",
    "        #Var(aX) = a**2 Var(X), so we need to multiply pred_sqrt_var = sigma (square root of the variance) by the standard normal distribution\n",
    "\n",
    "    def forward(self, x):\n",
    "        internal_repr  = self.encoder(x) # conv. part --> produces the internal representation\n",
    "        pred_means     = self.avg(internal_repr) # linear part 1 --> predicts means\n",
    "        pred_log_var   = self.log_var(internal_repr) # linear part 2 --> predicts variances\n",
    "        sample         = self.sample_in_latent_space(mu = pred_means, log_var = pred_log_var) # sample in latent space\n",
    "        decoded_sample = self.decoder(sample) # decoder --> produces final output\n",
    "        \n",
    "        return decoded_sample, pred_means, pred_log_var # we need to pass means and vars forward too, since they're needed to compute the KL div. term in the loss\n",
    "\n",
    "    def train_single_epoch(self, train_dataloader, verbose_single_epoch: bool = True): # function to be iterated inside the fit loop (no cv)\n",
    "        self.train() # enable dropout etc.\n",
    "        tr_err_single_epoch = 0\n",
    "        # SOLO BLURRED O PIXELATED\n",
    "        loading = tqdm(train_dataloader) if verbose_single_epoch else train_dataloader\n",
    "        for _, x_batch in loading: # ignore y_batch, we don't need the labels\n",
    "            output, mu, log_var = self(x_batch)\n",
    "            loss                = self.loss_fn(output, x_batch) + self.reg() - 0.5 * torch.sum(1. + log_var - mu**2 - torch.exp(log_var))\n",
    "            loss /= len(x_batch)\n",
    "            self.optimizer.zero_grad() # reset gradients\n",
    "            loss.backward() # backpropagation\n",
    "            self.optimizer.step() # update weights\n",
    "            tr_err_single_epoch += loss.detach().cpu().numpy() # save current training error\n",
    "        \n",
    "        tr_err_single_epoch /= len(train_dataloader) # len of a dataloader = n. of batches\n",
    "        return tr_err_single_epoch\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val_single_epoch(self, val_dataloader):\n",
    "        self.eval() # disable dropout etc.\n",
    "        val_err_single_epoch = 0\n",
    "\n",
    "        for _, x_batch in val_dataloader: # ignore y_batch, we don't need the labels\n",
    "            output, mu, log_var   = self(x_batch)\n",
    "            loss                  = nn.MSELoss()(output, x_batch) # we compare the reconstructed output with the original input\n",
    "            val_err_single_epoch += loss.detach().cpu().numpy()\n",
    "        \n",
    "        val_err_single_epoch /= len(val_dataloader)\n",
    "        return val_err_single_epoch\n",
    "\n",
    "    def fit(self, training_dataset, max_n_iter: int = 25, min_n_iter: int = 3, patience: int = 4, tol: float = 0.0001, seed: int = 1234, verbose: bool = True, num_workers: int = 0):\n",
    "        if seed is not None: # default seed to ensure reproducibility\n",
    "            torch.random.manual_seed(seed)\n",
    "\n",
    "        training_dataloader, validation_dataloader = create_train_val_dataloaders_b(training_dataset, batch_size = self.batch_size, num_workers = num_workers) # dependent on self.batch_size\n",
    "\n",
    "        self.training_error_history, self.val_error_history = np.zeros(max_n_iter), np.zeros(max_n_iter) # no CV --> no average over dataloaders --> we have a single value to use as an estimate of tr./val. errors\n",
    "        self.best_val_error = np.Inf\n",
    "        patience_counter = 0\n",
    "\n",
    "        loading = trange(max_n_iter) # if verbose else range(max_n_iter) # useful toggle during training/debugging\n",
    "        for epoch in loading:\n",
    "            if verbose:\n",
    "                print(f'Training {epoch=}') # useful to track training\n",
    "            tr_err  = self.training_error_history[epoch] = self.train_single_epoch(train_dataloader = training_dataloader)\n",
    "            val_err = self.val_error_history[epoch] = self.val_single_epoch(val_dataloader = validation_dataloader)\n",
    "            if verbose:\n",
    "                print(f'{epoch=}: {tr_err=}, {val_err=}') # useful to track training\n",
    "\n",
    "            if epoch > min_n_iter:\n",
    "                if val_err > self.best_val_error + tol:\n",
    "                    patience_counter += 1\n",
    "                else:\n",
    "                    self.best_val_error = val_err\n",
    "                    patience_counter = 0\n",
    "                if patience_counter > patience:\n",
    "                    break\n",
    "        n_executed_epochs = epoch + 1 # + 1 to go from 0, N-1 (python) to 1, N (human readable) counting scheme \n",
    "        if n_executed_epochs < max_n_iter:\n",
    "            self.best_val_error_epoch = n_executed_epochs - patience_counter\n",
    "        else:\n",
    "            self.best_val_error_epoch = max_n_iter - patience_counter\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, sample):\n",
    "        self.eval()\n",
    "        if sample.shape == torch.Size([3, 112, 112]): # a single sample # torch.Size([1, 28, 28]) in origine (-> serve la dim. 0 = N)\n",
    "            output = self(sample.unsqueeze(0))[0] # the forward pass computes means and variances too, but here we only want the reconstructed samples\n",
    "        else:\n",
    "            output = self(sample)[0]\n",
    "        return output\n",
    "\n",
    "    def test_accuracy(self, test_dataset):\n",
    "        original = test_dataset.cropped\n",
    "        try:\n",
    "            corrupted = test_dataset.blurred\n",
    "        except:\n",
    "            corrupted = test_dataset.pixelated\n",
    "        return float(nn.MSELoss()(self.predict(corrupted), original))\n",
    "\n",
    "    def plot_original_vs_reconstructed_sample(self, sample, figsize = (7,7), return_array: bool = False):\n",
    "        fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = figsize)\n",
    "        img = sample\n",
    "        # img_rec = self.predict(img).cpu().detach().squeeze(0).squeeze(0).numpy() # one to eliminate N_samples, one to eliminate N_channels\n",
    "        img_rec = self.predict(img).cpu().detach().squeeze(0).numpy().transpose((1, 2, 0))\n",
    "        # img = img.cpu().detach().squeeze(0).numpy()\n",
    "        img = img.cpu().detach().numpy().transpose((1, 2, 0))\n",
    "\n",
    "        ax[0].imshow(img, cmap = 'gray')\n",
    "        ax[0].set_title('Orig.')\n",
    "        ax[0].axis('off')\n",
    "        ax[1].imshow(img_rec, cmap = 'gray')\n",
    "        ax[1].set_title('Rec.')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        if return_array:\n",
    "            return img_rec\n",
    "        else:\n",
    "            return fig, ax\n",
    "\n",
    "    \n",
    "    def plot_reconstructed_samples(self, dataset, nrows: int = 3, ncols: int = 3, figsize = (42, 21)):\n",
    "        indices = np.arange(nrows*ncols)\n",
    "        idx = 0\n",
    "        ncols = 2*ncols\n",
    "        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = figsize)\n",
    "        \n",
    "        for i in range(nrows):\n",
    "            for j in range(0, ncols, 2):\n",
    "                img = dataset.data[indices[idx]]\n",
    "                img_rec = self.plot_original_vs_reconstructed_sample(img, return_array = True)\n",
    "                # img = img.detach().cpu().squeeze(0).numpy()\n",
    "                img = img.detach().cpu().numpy().transpose((1, 2, 0))\n",
    "                ax[i,j].imshow(img, cmap = 'gray')\n",
    "                ax[i,j].set_title('Orig.')\n",
    "                ax[i,j].axis('off')\n",
    "                ax[i,j+1].imshow(img_rec, cmap = 'gray')\n",
    "                ax[i,j+1].set_title('Rec.')\n",
    "                ax[i,j+1].axis('off')\n",
    "                idx += 1\n",
    "\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set.cropped = training_set.cropped.to('cpu')\n",
    "# training_set.blurred = training_set.blurred.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VarAutoEncoder(64, 'ReLU', 'Adam', 1e-3, 128)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 112, 112])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.cropped[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = training_set.cropped[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model(a)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.empty((1, 32, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 9, 9])\n",
      "torch.Size([1, 16, 19, 19])\n",
      "torch.Size([1, 8, 37, 37])\n",
      "torch.Size([1, 3, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "z1 = nn.ConvTranspose2d(32, 32, kernel_size = 3, stride = 2, output_padding = 0)(z)\n",
    "print(z1.shape)\n",
    "z2 = nn.ConvTranspose2d(32, 16, kernel_size = 3, stride = 2, output_padding = 0)(z1)\n",
    "print(z2.shape)\n",
    "z3 = nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 2, padding = 1, output_padding = 0)(z2)\n",
    "print(z3.shape)\n",
    "z4 = nn.ConvTranspose2d(8, 3, kernel_size = 4, stride = 3, padding = 1, output_padding = 2)(z3)\n",
    "print(z4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ConvTranspose2d(32, 16, kernel_size = 3, stride = 2, output_padding = 0),\n",
    "nn.ConvTranspose2d(16, 8, kernel_size = 3, stride = 2, padding = 1, output_padding = 0),\n",
    "nn.ConvTranspose2d(8, 3, kernel_size = 3, stride = 2, padding = 0, output_padding = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 75, 75])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:21<00:00,  3.10it/s]\n",
      "  4%|▍         | 1/25 [00:23<09:26, 23.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: tr_err=array([1624.54515719]), val_err=0.03584180465515922\n",
      "Training epoch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.64it/s]\n",
      "  8%|▊         | 2/25 [00:43<08:19, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: tr_err=array([1231.99766686]), val_err=0.028168122119763318\n",
      "Training epoch=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.63it/s]\n",
      " 12%|█▏        | 3/25 [01:04<07:44, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: tr_err=array([860.44919476]), val_err=0.01658227782258216\n",
      "Training epoch=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.66it/s]\n",
      " 16%|█▌        | 4/25 [01:24<07:16, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: tr_err=array([585.69971928]), val_err=0.014921854743186165\n",
      "Training epoch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.63it/s]\n",
      " 20%|██        | 5/25 [01:45<06:52, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: tr_err=array([517.06675769]), val_err=0.011850555372588775\n",
      "Training epoch=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.58it/s]\n",
      " 24%|██▍       | 6/25 [02:05<06:32, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: tr_err=array([466.2190589]), val_err=0.01064763470169376\n",
      "Training epoch=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.65it/s]\n",
      " 28%|██▊       | 7/25 [02:26<06:10, 20.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: tr_err=array([440.99903326]), val_err=0.010193193933981307\n",
      "Training epoch=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.59it/s]\n",
      " 32%|███▏      | 8/25 [02:46<05:50, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: tr_err=array([425.03187214]), val_err=0.009747018106281757\n",
      "Training epoch=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.59it/s]\n",
      " 36%|███▌      | 9/25 [03:07<05:30, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: tr_err=array([407.0350564]), val_err=0.009530601749087082\n",
      "Training epoch=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.56it/s]\n",
      " 40%|████      | 10/25 [03:28<05:10, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: tr_err=array([391.99341701]), val_err=0.009074156556059332\n",
      "Training epoch=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.54it/s]\n",
      " 44%|████▍     | 11/25 [03:49<04:50, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=10: tr_err=array([376.18165498]), val_err=0.00836055654594127\n",
      "Training epoch=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.50it/s]\n",
      " 48%|████▊     | 12/25 [04:10<04:31, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=11: tr_err=array([358.99870939]), val_err=0.008214823600343046\n",
      "Training epoch=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.57it/s]\n",
      " 52%|█████▏    | 13/25 [04:31<04:10, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=12: tr_err=array([352.69969041]), val_err=0.007997550672906287\n",
      "Training epoch=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.52it/s]\n",
      " 56%|█████▌    | 14/25 [04:52<03:49, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=13: tr_err=array([342.11247726]), val_err=0.00813441605800215\n",
      "Training epoch=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.60it/s]\n",
      " 60%|██████    | 15/25 [05:12<03:27, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=14: tr_err=array([336.44464479]), val_err=0.007610452005310971\n",
      "Training epoch=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.56it/s]\n",
      " 64%|██████▍   | 16/25 [05:33<03:07, 20.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=15: tr_err=array([330.00454987]), val_err=0.0075379093451535\n",
      "Training epoch=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:18<00:00,  3.59it/s]\n",
      " 68%|██████▊   | 17/25 [05:54<02:46, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=16: tr_err=array([325.37611954]), val_err=0.007394536672269597\n",
      "Training epoch=17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.50it/s]\n",
      " 72%|███████▏  | 18/25 [06:15<02:26, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=17: tr_err=array([320.58769783]), val_err=0.007143169093657942\n",
      "Training epoch=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.50it/s]\n",
      " 76%|███████▌  | 19/25 [06:36<02:05, 20.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=18: tr_err=array([312.22545547]), val_err=0.007143096833982889\n",
      "Training epoch=19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.44it/s]\n",
      " 80%|████████  | 20/25 [06:58<01:45, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=19: tr_err=array([310.88268111]), val_err=0.006940418580437408\n",
      "Training epoch=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.38it/s]\n",
      " 84%|████████▍ | 21/25 [07:19<01:25, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=20: tr_err=array([303.15243232]), val_err=0.0069327530560686305\n",
      "Training epoch=21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:20<00:00,  3.29it/s]\n",
      " 88%|████████▊ | 22/25 [07:42<01:04, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=21: tr_err=array([296.93699409]), val_err=0.006886642481036046\n",
      "Training epoch=22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.48it/s]\n",
      " 92%|█████████▏| 23/25 [08:03<00:43, 21.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=22: tr_err=array([295.02754308]), val_err=0.0066125574795638815\n",
      "Training epoch=23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.47it/s]\n",
      " 96%|█████████▌| 24/25 [08:24<00:21, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=23: tr_err=array([290.21534734]), val_err=0.006385253687553546\n",
      "Training epoch=24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:19<00:00,  3.46it/s]\n",
      "100%|██████████| 25/25 [08:46<00:00, 21.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=24: tr_err=array([285.81497525]), val_err=0.006389781312250039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8eb363284fb1694869db5263c0d7857a73fbcd06182a3163e391c8e62d44dc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
